import sys
import math
import os
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn import preprocessing
from sklearn.cluster import AffinityPropagation
from sklearn.metrics import silhouette_samples, silhouette_score

__author__ = 'Paolo'


class dp_means():
	def __init__(self, _X, _k = 1, _lam = 1, _stop=False):
		self.lam = _lam
		self.nFeatures = len(_X[0])
		self.size = len(_X)
		self.X = _X
		self.stop = _stop
		# Initialize group membership
		self.dataClusterId = [-1 for i in range(0, self.size)] # index of group for each data pair
		# clusters' centers
		self.clusters = {}
		# output records
		self.record = []
		self.errorRecord = []
		self.k = _k

	def dSquared(self, x, y):
		dist2 = 0.0
		for j,k in zip(x,y):
			dist2 += (j - k)**2
		return dist2

	def error(self):
		res = 0.0
		for i in range(0, self.size):
			res += self.dSquared(self.X[i], self.clusters[self.dataClusterId[i]])
		return res/self.size

	def nearestCluster(self, x):
		cmin = sys.maxint
		cidx = -sys.maxint
		for j in self.clusters:
			dist = math.sqrt(self.dSquared(x, self.clusters[j]))
			if dist < cmin:  # record closest centroid
				cmin = dist
				cidx = j
		return cidx, cmin

	def assign(self):
		for i in range(0, self.allSize):
			self.dataClusterId[i], dmin = self.nearestCluster(self.X[i])

	def updateClusters(self):
		ctemp = {} # dim sums by cluster
		for j in range(0, self.k):
			ctemp[j] = []
			for k in range(0, self.nFeatures):
				ctemp[j].append(0.0) # init sums
			ctemp[j].append(0) # init counter
		# only calculate clusters on training, not cross-validation set
		for i in range(0,self.size):
			for j in range(0, self.nFeatures):
				ctemp[self.dataClusterId[i]][j] += self.X[i][j]
			ctemp[self.dataClusterId[i]][self.nFeatures] += 1 # count
		for c in self.clusters:
			if ctemp[c][self.nFeatures] <> 0:
				self.clusters[c] = [ ctemp[c][k]/ctemp[c][self.nFeatures] for k in range(0,self.nFeatures)]
			else:
				# no members in this cluster
				pass
		return

	def assign(self):
		for i in range(0, self.size):
			cidx, dmin = self.nearestCluster(self.X[i])
			if dmin > self.lam:
				self.k += 1
				self.clusters[self.k-1] = self.X[i]
				self.dataClusterId[i] = self.k - 1
			else:
				self.dataClusterId[i] = cidx

	def run(self, nmax = 100, eps = 1e-7):
		prev = 0.0
		for iter in range(0, nmax):
			# update assignments
			self.assign()
			# calculate error
			#err = self.error()
			#
			#if abs(err-prev) < eps:
			#	sys.stderr.write("Tolerance reached at step %d\n"%iter)
			#	break
			#prev = err
			# going on...
			#self.errorRecord.append((iter, err))
			#self.output(str(iter))
			self.updateClusters()
		#sys.stderr.write("Iterations completed: %d\n"%iter)
		#sys.stderr.write("Final error: %f\n"%prev)
		# This is a step past stop if using cross-validation...
		#self.output("Final")
		#return err

	def output(self, iter):
		for i in range(0,self.size):
			self.record.append([str(y) for y in self.X[i]] + [str(self.dataClusterId[i])] + ["Iter-%s"%iter])
		for k in self.clusters:
			self.record.append([str(y) for y in self.clusters[k]] + [str(k)] + ["Cent-Iter-%s"%iter])

	def getOutput(self):
		for x in self.record:
			yield x

	def getErrors(self):
		for x in self.errorRecord:
			yield x


def main_old_data():

	from data_import.ImportDataset import ImportData
	from sklearn.metrics.pairwise import pairwise_distances_argmin

	importer = ImportData('C:/Users/Paolo/Desktop/Reply/Thesis/Data/XsenseData/MT_07700161-003-001.txt')
	data = importer.import_csv_reduced()
	X =importer.get_clustering_table(data)

	estimators_dpmeans = {'dp_means_5': dp_means(X, _lam=5)}
						  # 'dp_means_5': dp_means(X, _lam=5),
						  #'dp_means_5': dp_means(X, _lam=10)}

	for name, est in estimators_dpmeans.items():
		est.run()
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print est.clusters.values()

	estimators_kmeans = {'k_means_k': KMeans(n_clusters=k)}
				#'k_means_15': KMeans(n_clusters=15),
				#'k_means_20': KMeans(n_clusters=20)}
	for name, est in estimators_kmeans.items():
		est.fit(X)
		est.predict(X)
		print name
		print est.labels_
		print est.cluster_centers_
		""" Per ciascun dei centroidi del dpmeans trova quello piu vicino """
	order = pairwise_distances_argmin(estimators_dpmeans['dp_means_5'].clusters.values(),
									  estimators_kmeans['k_means_k'].cluster_centers_)
	print order


def plot_silhouette(s_score, s_values, n_clusters, cluster_labels, X):

	import matplotlib.pyplot as plt
	import matplotlib.cm as cm

	fig, (ax1) = plt.subplots(1, 1)
	fig.set_size_inches(18, 7)

	ax1.set_xlim([-1, 1])
	ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

	y_lower = 10
	for i in range(1,n_clusters+1):
		ith_cluster_silhouette_values = \
			s_values[cluster_labels == i]

		ith_cluster_silhouette_values.sort()

		size_cluster_i = ith_cluster_silhouette_values.shape[0]
		y_upper = y_lower + size_cluster_i

		color = cm.spectral(float(i) / n_clusters)
		ax1.fill_betweenx(np.arange(y_lower, y_upper),
						  0, ith_cluster_silhouette_values,
						  facecolor=color, edgecolor=color, alpha=0.7)

		# Label the silhouette plots with their cluster numbers at the middle
		ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

		# Compute the new y_lower for next plot
		y_lower = y_upper + 10  # 10 for the 0 samples

	ax1.set_title("The silhouette plot for the various clusters.")
	ax1.set_xlabel("The silhouette coefficient values")
	ax1.set_ylabel("Cluster label")

	ax1.axvline(x=s_score, color="red", linestyle="--")
	ax1.set_yticks([])  # Clear the yaxis labels / ticks
	ax1.set_xticks([-1,-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])

	plt.suptitle(("Silhouette analysis for clustering on sample data "
				  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')
	plt.show()


def save_csv_clustering_results(data, file_name):
	"""
	:param data: dataframe to be written, last column contains clustering labels
	:param file_name: name of the file
	:return: null
	"""
	format = '.pdf'
	dir_name = '/data_clustering/'
	full_path = os.path.join(dir_name, file_name + format)
	data.to_csv(full_path, sep=';', index=False)


def main():
	from data_import.ImportDataset import ImportData

	""" Analisi  tramite clustering su dataset globale"""
	""" No diff_Yaw """
	""" Tuning valore di lambda basato su silhouette coefficient"""

	importer = ImportData('') # Non importa il path da rimuovere, inutile
	data_full = pd.read_csv('../xsense_data/global_dataset.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y']]
	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()
	#print 'X', X
	X_np = data_clustering.values
	#print 'X_np', X_np
	print 'reduced'

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)


	estimators_dpmeans = {
	# 					  'dp_means_5': dp_means(X_scaled, _lam=5),
	#					  'dp_means_7': dp_means(X_scaled, _lam=7),
	 					  'dp_means_10': dp_means(X_scaled, _lam=12)
	#					  'dp_means_20': dp_means(X_scaled, _lam=20),
	#					  'dp_means_12': dp_means(X_scaled, _lam=12)
						 }

	for name, est in estimators_dpmeans.items():
		print 'start dpmeans run'
		est.run()
		print 'finished dpmeans run'
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print std_scale.inverse_transform(est.clusters.values())

		""" Add cluster assignments to dataset """
		data_clustering.loc[:, 'Cluster_Label'] = pd.Series(np.array(est.dataClusterId), index= data_clustering.index)
		#(TODO): Save clustered dataset in file

		#Extract just one trip from the full dataset to compute silhouette scores
		trip_id = 15
		df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

		data_clustering_trip = df_trip[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y']]
		print 'data Clustering trip', data_clustering_trip

		X_trip = data_clustering_trip.as_matrix()

		data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
		labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

		print "labels_trip", labels_trip
		print "np array labels trip",  np.array(labels_trip)
		silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

		silhouette_avg = silhouette_score(X, np.array(est.dataClusterId), sample_size= 5000)
		#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

		#print 'Silhouette score ', name, ': ', silhouette_avg

		plot_silhouette(silhouette_avg, silhouette_values_trip, k, np.array(labels_trip), X_trip)


def cluster_abs_diff_yaw():
	import seaborn as sns
	import matplotlib.pyplot as plt

	""" Analisi  tramite clustering su dataset globale con abs su velocita e delta yaw """
	""" Tuning valore di lambda basato su silhouette coefficient """

	data_full = pd.read_csv('../xsense_data/global_dataset_abs_speed_diff_yaw.txt', sep=';')
	print 'imported'

	#(TODO): Reuse Yaw before commmit
	#data_clustering = data_full[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y','Diff_Yaw']]
	data_clustering = data_full[['Acc_X','Acc_Y','Speed_X','Speed_Y','Diff_Yaw']]

	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()
	#print 'X', X
	X_np = data_clustering.values
	#print 'X_np', X_np
	print 'reduced'

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)

	#(TODO): CReate a separeted method
	""" Correlation analysis """
	corr = data_clustering.corr()
	sns.heatmap(corr,square = True)
	plt.yticks(rotation=0)
	plt.xticks(rotation=90)
	plt.show()

	estimators_dpmeans = {
	# 					  'dp_means_5': dp_means(X_scaled, _lam=5),
	#					  'dp_means_7': dp_means(X_scaled, _lam=7),
	 					  'dp_means_10': dp_means(X_scaled, _lam=10),
						  'dp_means_12': dp_means(X_scaled, _lam=12)
	#					  'dp_means_20': dp_means(X_scaled, _lam=20)
						 }

	for name, est in estimators_dpmeans.items():
		print 'start dpmeans run'
		est.run()
		print 'finished dpmeans run'
		k = len(np.unique(est.dataClusterId))
		print 'n_cluster : ',len(np.unique(est.dataClusterId))
		print 'model : ',name
		print 'labels : ',est.dataClusterId
		print 'centroids : ',std_scale.inverse_transform(est.clusters.values())

		""" Add cluster assignments to dataset """
		data_clustering.loc[:, 'Cluster_Label'] = pd.Series(np.array(est.dataClusterId), index= data_clustering.index)
		#(TODO): Save clustered dataset in file

		#Extract just one trip from the full dataset to compute silhouette scores
		trip_id = 26
		df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

		#(TODO): Reuse Yaw before commmit

		data_clustering_trip = df_trip[['Acc_X','Acc_Y','Speed_X','Speed_Y', 'Diff_Yaw']]
		#data_clustering_trip = df_trip[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y', 'Diff_Yaw']]
		#print 'data Clustering trip', data_clustering_trip

		X_trip = data_clustering_trip.as_matrix()

		data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
		labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

		print "labels_trip", labels_trip
		print "np array labels trip",  np.array(labels_trip)
		silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

		silhouette_avg = silhouette_score(X, np.array(est.dataClusterId), sample_size= 5000)
		#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

		#print 'Silhouette score ', name, ': ', silhouette_avg

		plot_silhouette(silhouette_avg, silhouette_values_trip, k, np.array(labels_trip), X_trip)


def k_means_clustering(n_cluster):

	data_full = pd.read_csv('../xsense_data/global_dataset_abs_speed_diff_yaw.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X','Acc_Y','Speed_X','Speed_Y','Diff_Yaw']]
	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)
	print 'scaled'

	""" KMeans clustering algorithm"""
	k_means_model = KMeans(n_clusters= n_cluster, init='k-means++')
	k_labels = k_means_model.fit_predict(X_scaled)
	k_centroids = k_means_model.cluster_centers_

	print 'Centroids: ', k_centroids
	""" Add cluster assignments to dataset """
	data_clustering.loc[:, 'Cluster_Label'] = pd.Series(k_labels, index= data_clustering.index)
	#(TODO): Save clustered dataset in file

	#Extract just one trip from the full dataset to compute silhouette scores
	trip_id = 26
	df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

	data_clustering_trip = df_trip[['Acc_X','Acc_Y','Speed_X','Speed_Y', 'Diff_Yaw']]
	#print 'data Clustering trip', data_clustering_trip

	X_trip = data_clustering_trip.as_matrix()

	data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
	labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

	print "labels_trip", labels_trip
	print "np array labels trip",  np.array(labels_trip)
	silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

	silhouette_avg = silhouette_score(X, k_labels, sample_size= 10000)
	#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

	#print 'Silhouette score ', name, ': ', silhouette_avg

	plot_silhouette(silhouette_avg, silhouette_values_trip, n_cluster, np.array(labels_trip), X_trip)


def affinity_propagation_xsens_dataset():

	#Extract just one trip from the full dataset to compute silhouette scores
	trip_id = 26

	data_full = pd.read_csv('../xsense_data/global_dataset_abs_speed_diff_yaw.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X','Acc_Y','Speed_X','Speed_Y','Diff_Yaw']]
	data_clustering = data_clustering.round(5)
	data_clustering = data_clustering.loc[data_full['Trip_ID'] == trip_id]

	X = data_clustering.as_matrix()

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)
	print 'scaled'

	affinity_model = AffinityPropagation(damping = 0.8, preference= -300).fit(X_scaled)
	cluster_centers_indices = affinity_model.cluster_centers_indices_

	labels = affinity_model.labels_
	n_clusters = len(cluster_centers_indices)

	print 'labels: ', labels
	print 'n_clusters: ', n_clusters

	""" Add cluster assignments to dataset """
	data_clustering.loc[:, 'Cluster_Label'] = pd.Series(labels, index= data_clustering.index)
	#(TODO): Save clustered dataset in file

	df_trip = data_full.loc[data_full['Trip_ID'] == trip_id]

	data_clustering_trip = df_trip[['Acc_X','Acc_Y','Speed_X','Speed_Y', 'Diff_Yaw']]
	#print 'data Clustering trip', data_clustering_trip

	X_trip = data_clustering_trip.as_matrix()

	data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
	labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

	print "labels_trip", labels_trip
	print "np array labels trip",  np.array(labels_trip)
	silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

	silhouette_avg = silhouette_score(X_scaled, labels, sample_size= 10000)
	#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

	#print 'Silhouette score ', name, ': ', silhouette_avg

	plot_silhouette(silhouette_avg, silhouette_values_trip, n_clusters, np.array(labels_trip), X_trip)


def dp_means_hcilab_dataset():
	import seaborn as sns
	import matplotlib.pyplot as plt

	""" Analisi  tramite clustering su dataset globale con abs su velocita e delta yaw """
	""" Tuning valore di lambda basato su silhouette coefficient """

	data_full = pd.read_csv('../xsense_data/global_dataset_hcilab_diff_bearing.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]

	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()
	#print 'X', X
	X_np = data_clustering.values
	#print 'X_np', X_np
	print 'reduced'

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)

	#(TODO): CReate a separeted method
	""" Correlation analysis """
	corr = data_clustering.corr()
	sns.heatmap(corr,square = True)
	plt.yticks(rotation=0)
	plt.xticks(rotation=90)
	plt.show()

	estimators_dpmeans = {
	 					  'dp_means_6': dp_means(X_scaled, _lam=6),
						  'dp_means_8': dp_means(X_scaled, _lam=8),
	 					  'dp_means_9': dp_means(X_scaled, _lam=9)
	#					  'dp_means_12': dp_means(X_scaled, _lam=12)
	#					  'dp_means_20': dp_means(X_scaled, _lam=20)
						 }

	for name, est in estimators_dpmeans.items():
		print 'start dpmeans run'
		est.run()
		print 'finished dpmeans run'
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print std_scale.inverse_transform(est.clusters.values())

		""" Add cluster assignments to dataset """
		data_clustering.loc[:, 'Cluster_Label'] = pd.Series(np.array(est.dataClusterId), index= data_clustering.index)
		#(TODO): Save clustered dataset in file

		#Extract just one trip from the full dataset to compute silhouette scores
		trip_id = 5
		df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

		#(TODO): Reuse Yaw before commmit

		data_clustering_trip = df_trip[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]

		X_trip = data_clustering_trip.as_matrix()

		data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
		labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

		print "labels_trip", labels_trip
		print "np array labels trip",  np.array(labels_trip)
		silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

		silhouette_avg = silhouette_score(X, np.array(est.dataClusterId), sample_size= 5000)
		#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

		print 'Silhouette score ', name, ': ', silhouette_avg

		plot_silhouette(silhouette_avg, silhouette_values_trip, k, np.array(labels_trip), X_trip)


def affinity_propagation_hcilab_dataset():
	trip_id = 5

	data_full = pd.read_csv('../xsense_data/global_dataset_hcilab_diff_bearing.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]
	data_clustering = data_clustering.round(5)
	data_clustering = data_clustering.loc[data_full['Trip_ID'] == trip_id]

	X = data_clustering.as_matrix()

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)
	print 'scaled'

	affinity_model = AffinityPropagation(damping = 0.8, preference= -200).fit(X_scaled)
	cluster_centers_indices = affinity_model.cluster_centers_indices_

	labels = affinity_model.labels_
	n_clusters = len(cluster_centers_indices)
	centroids = std_scale.inverse_transform(affinity_model.cluster_centers_)

	print 'labels: ', labels
	print 'n_clusters: ', n_clusters
	print 'centroids: ', centroids

	""" Add cluster assignments to dataset """
	data_clustering.loc[:, 'Cluster_Label'] = pd.Series(labels, index= data_clustering.index)
	#(TODO): Save clustered dataset in file

	df_trip = data_full.loc[data_full['Trip_ID'] == trip_id]

	data_clustering_trip = df_trip[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]
	#print 'data Clustering trip', data_clustering_trip

	X_trip = data_clustering_trip.as_matrix()

	data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
	labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

	print "labels_trip", labels_trip
	silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

	silhouette_avg = silhouette_score(X_scaled, labels, sample_size= 10000)
	#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

	print 'Silhouette score : ', silhouette_avg

	plot_silhouette(silhouette_avg, silhouette_values_trip, n_clusters, np.array(labels_trip), X_trip)


if __name__ == "__main__":
	#main()
	#cluster_abs_diff_yaw()
	#k_means_clustering(5)
	#k_means_clustering(7)
	#k_means_clustering(9)

	#affinity_propagation_xsens_dataset()

	#dp_means_hcilab_dataset()
	affinity_propagation_hcilab_dataset()