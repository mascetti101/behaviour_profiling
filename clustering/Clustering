import sys
import math
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn import preprocessing
from sklearn.metrics import silhouette_samples, silhouette_score

__author__ = 'Paolo'

class dp_means():
	def __init__(self, _X, _k = 1, _lam = 1, _stop=False):
		self.lam = _lam
		self.nFeatures = len(_X[0])
		self.size = len(_X)
		self.X = _X
		self.stop = _stop
		# Initialize group membership
		self.dataClusterId = [-1 for i in range(0, self.size)] # index of group for each data pair
		# clusters' centers
		self.clusters = {}
		# output records
		self.record = []
		self.errorRecord = []
		self.k = _k

	def dSquared(self, x, y):
		dist2 = 0.0
		for j,k in zip(x,y):
			dist2 += (j - k)**2
		return dist2

	def error(self):
		res = 0.0
		for i in range(0, self.size):
			res += self.dSquared(self.X[i], self.clusters[self.dataClusterId[i]])
		return res/self.size

	def nearestCluster(self, x):
		cmin = sys.maxint
		cidx = -sys.maxint
		for j in self.clusters:
			dist = math.sqrt(self.dSquared(x, self.clusters[j]))
			if dist < cmin:  # record closest centroid
				cmin = dist
				cidx = j
		return cidx, cmin

	def assign(self):
		for i in range(0, self.allSize):
			self.dataClusterId[i], dmin = self.nearestCluster(self.X[i])

	def updateClusters(self):
		ctemp = {} # dim sums by cluster
		for j in range(0, self.k):
			ctemp[j] = []
			for k in range(0, self.nFeatures):
				ctemp[j].append(0.0) # init sums
			ctemp[j].append(0) # init counter
		# only calculate clusters on training, not cross-validation set
		for i in range(0,self.size):
			for j in range(0, self.nFeatures):
				ctemp[self.dataClusterId[i]][j] += self.X[i][j]
			ctemp[self.dataClusterId[i]][self.nFeatures] += 1 # count
		for c in self.clusters:
			if ctemp[c][self.nFeatures] <> 0:
				self.clusters[c] = [ ctemp[c][k]/ctemp[c][self.nFeatures] for k in range(0,self.nFeatures)]
			else:
				# no members in this cluster
				pass
		return

	def assign(self):
		for i in range(0, self.size):
			cidx, dmin = self.nearestCluster(self.X[i])
			if dmin > self.lam:
				self.k += 1
				self.clusters[self.k-1] = self.X[i]
				self.dataClusterId[i] = self.k - 1
			else:
				self.dataClusterId[i] = cidx

	def run(self, nmax = 100, eps = 1e-7):
		prev = 0.0
		for iter in range(0, nmax):
			# update assignments
			self.assign()
			# calculate error
			#err = self.error()
			#
			#if abs(err-prev) < eps:
			#	sys.stderr.write("Tolerance reached at step %d\n"%iter)
			#	break
			#prev = err
			# going on...
			#self.errorRecord.append((iter, err))
			#self.output(str(iter))
			self.updateClusters()
		#sys.stderr.write("Iterations completed: %d\n"%iter)
		#sys.stderr.write("Final error: %f\n"%prev)
		# This is a step past stop if using cross-validation...
		#self.output("Final")
		#return err

	def output(self, iter):
		for i in range(0,self.size):
			self.record.append([str(y) for y in self.X[i]] + [str(self.dataClusterId[i])] + ["Iter-%s"%iter])
		for k in self.clusters:
			self.record.append([str(y) for y in self.clusters[k]] + [str(k)] + ["Cent-Iter-%s"%iter])

	def getOutput(self):
		for x in self.record:
			yield x

	def getErrors(self):
		for x in self.errorRecord:
			yield x

def main_old_data():

	from data_import.ImportDataset import ImportData
	from sklearn.metrics.pairwise import pairwise_distances_argmin

	importer = ImportData('C:/Users/Paolo/Desktop/Reply/Thesis/Data/XsenseData/MT_07700161-003-001.txt')
	data = importer.import_csv_reduced()
	X =importer.get_clustering_table(data)

	estimators_dpmeans = {'dp_means_5': dp_means(X, _lam=5)}
						  # 'dp_means_5': dp_means(X, _lam=5),
						  #'dp_means_5': dp_means(X, _lam=10)}

	for name, est in estimators_dpmeans.items():
		est.run()
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print est.clusters.values()

	estimators_kmeans = {'k_means_k': KMeans(n_clusters=k)}
				#'k_means_15': KMeans(n_clusters=15),
				#'k_means_20': KMeans(n_clusters=20)}
	for name, est in estimators_kmeans.items():
		est.fit(X)
		est.predict(X)
		print name
		print est.labels_
		print est.cluster_centers_
		""" Per ciascun dei centroidi del dpmeans trova quello piu vicino """
	order = pairwise_distances_argmin(estimators_dpmeans['dp_means_5'].clusters.values(),
									  estimators_kmeans['k_means_k'].cluster_centers_)
	print order


def plot_silhouette(s_score, s_values, n_clusters, cluster_labels, X):

	import matplotlib.pyplot as plt
	import matplotlib.cm as cm

	fig, (ax1) = plt.subplots(1, 1)
	fig.set_size_inches(18, 7)

	ax1.set_xlim([-1, 1])
	ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

	y_lower = 10
	for i in range(1,n_clusters+1):
		ith_cluster_silhouette_values = \
			s_values[cluster_labels == i]

		ith_cluster_silhouette_values.sort()

		size_cluster_i = ith_cluster_silhouette_values.shape[0]
		y_upper = y_lower + size_cluster_i

		color = cm.spectral(float(i) / n_clusters)
		ax1.fill_betweenx(np.arange(y_lower, y_upper),
						  0, ith_cluster_silhouette_values,
						  facecolor=color, edgecolor=color, alpha=0.7)

		# Label the silhouette plots with their cluster numbers at the middle
		ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

		# Compute the new y_lower for next plot
		y_lower = y_upper + 10  # 10 for the 0 samples

	ax1.set_title("The silhouette plot for the various clusters.")
	ax1.set_xlabel("The silhouette coefficient values")
	ax1.set_ylabel("Cluster label")

	ax1.axvline(x=s_score, color="red", linestyle="--")
	ax1.set_yticks([])  # Clear the yaxis labels / ticks
	ax1.set_xticks([-1,-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])

	plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
				  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')
	plt.show()


def main():
	from data_import.ImportDataset import ImportData

	""" Analisi  tramite clustering su dataset globale"""
	""" Tuning valore di lambda basato su silhouette coefficient"""

	importer = ImportData('') # Non importa il path da rimuovere, inutile
	data_full = pd.read_csv('../xsense_data/global_dataset.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y']]
	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()
	#print 'X', X
	X_np = data_clustering.values
	#print 'X_np', X_np
	print 'reduced'

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)


	estimators_dpmeans = {
	# 					  'dp_means_5': dp_means(X_scaled, _lam=5),
	#					  'dp_means_7': dp_means(X_scaled, _lam=7),
	 					  'dp_means_10': dp_means(X_scaled, _lam=12)
	#					  'dp_means_20': dp_means(X_scaled, _lam=20),
	#					  'dp_means_12': dp_means(X_scaled, _lam=12)
						 }

	for name, est in estimators_dpmeans.items():
		print 'start dpmeans run'
		est.run()
		print 'finished dpmeans run'
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print std_scale.inverse_transform(est.clusters.values())

		""" Add cluster assignments to dataset """
		data_clustering.loc[:, 'Cluster_Label'] = pd.Series(np.array(est.dataClusterId), index= data_clustering.index)
		#(TODO): Save clustered dataset in file

		#Extract just one trip from the full dataset to compute silhouette scores
		trip_id = 15
		df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

		data_clustering_trip = df_trip[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y']]
		print 'data Clustering trip', data_clustering_trip

		X_trip = data_clustering_trip.as_matrix()

		data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
		labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

		print "labels_trip", labels_trip
		print "np array labels trip",  np.array(labels_trip)
		silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

		silhouette_avg = silhouette_score(X, np.array(est.dataClusterId), sample_size= 5000)
		#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

		#print 'Silhouette score ', name, ': ', silhouette_avg

		plot_silhouette(silhouette_avg, silhouette_values_trip, k, np.array(labels_trip), X_trip)

if __name__ == "__main__":
	main()