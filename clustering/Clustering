import sys
import math
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn import preprocessing
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_samples, silhouette_score

__author__ = 'Paolo'

class dp_means():
	def __init__(self, _X, _k = 1, _lam = 1, _stop=False):
		self.lam = _lam
		self.nFeatures = len(_X[0])
		self.size = len(_X)
		self.X = _X
		self.stop = _stop
		# Initialize group membership
		self.dataClusterId = [-1 for i in range(0, self.size)] # index of group for each data pair
		# clusters' centers
		self.clusters = {}
		# output records
		self.record = []
		self.errorRecord = []
		self.k = _k

	def dSquared(self, x, y):
		dist2 = 0.0
		for j,k in zip(x,y):
			dist2 += (j - k)**2
		return dist2

	def error(self):
		res = 0.0
		for i in range(0, self.size):
			res += self.dSquared(self.X[i], self.clusters[self.dataClusterId[i]])
		return res/self.size

	def nearestCluster(self, x):
		cmin = sys.maxint
		cidx = -sys.maxint
		for j in self.clusters:
			dist = math.sqrt(self.dSquared(x, self.clusters[j]))
			if dist < cmin:  # record closest centroid
				cmin = dist
				cidx = j
		return cidx, cmin

	def assign(self):
		for i in range(0, self.allSize):
			self.dataClusterId[i], dmin = self.nearestCluster(self.X[i])

	def updateClusters(self):
		ctemp = {} # dim sums by cluster
		for j in range(0, self.k):
			ctemp[j] = []
			for k in range(0, self.nFeatures):
				ctemp[j].append(0.0) # init sums
			ctemp[j].append(0) # init counter
		# only calculate clusters on training, not cross-validation set
		for i in range(0,self.size):
			for j in range(0, self.nFeatures):
				ctemp[self.dataClusterId[i]][j] += self.X[i][j]
			ctemp[self.dataClusterId[i]][self.nFeatures] += 1 # count
		for c in self.clusters:
			if ctemp[c][self.nFeatures] <> 0:
				self.clusters[c] = [ ctemp[c][k]/ctemp[c][self.nFeatures] for k in range(0,self.nFeatures)]
			else:
				# no members in this cluster
				pass
		return

	def assign(self):
		for i in range(0, self.size):
			cidx, dmin = self.nearestCluster(self.X[i])
			if dmin > self.lam:
				self.k += 1
				self.clusters[self.k-1] = self.X[i]
				self.dataClusterId[i] = self.k - 1
			else:
				self.dataClusterId[i] = cidx

	def run(self, nmax = 100, eps = 1e-7):
		prev = 0.0
		for iter in range(0, nmax):
			# update assignments
			self.assign()
			# calculate error
			#err = self.error()
			#
			#if abs(err-prev) < eps:
			#	sys.stderr.write("Tolerance reached at step %d\n"%iter)
			#	break
			#prev = err
			# going on...
			#self.errorRecord.append((iter, err))
			#self.output(str(iter))
			self.updateClusters()
		#sys.stderr.write("Iterations completed: %d\n"%iter)
		#sys.stderr.write("Final error: %f\n"%prev)
		# This is a step past stop if using cross-validation...
		#self.output("Final")
		#return err

	def output(self, iter):
		for i in range(0,self.size):
			self.record.append([str(y) for y in self.X[i]] + [str(self.dataClusterId[i])] + ["Iter-%s"%iter])
		for k in self.clusters:
			self.record.append([str(y) for y in self.clusters[k]] + [str(k)] + ["Cent-Iter-%s"%iter])

	def getOutput(self):
		for x in self.record:
			yield x

	def getErrors(self):
		for x in self.errorRecord:
			yield x

def main_old_data():

	from data_import.ImportDataset import ImportData
	from sklearn.metrics.pairwise import pairwise_distances_argmin

	importer = ImportData('C:/Users/Paolo/Desktop/Reply/Thesis/Data/XsenseData/MT_07700161-003-001.txt')
	data = importer.import_csv_reduced()
	X =importer.get_clustering_table(data)

	estimators_dpmeans = {'dp_means_5': dp_means(X, _lam=5)}
						  # 'dp_means_5': dp_means(X, _lam=5),
						  #'dp_means_5': dp_means(X, _lam=10)}

	for name, est in estimators_dpmeans.items():
		est.run()
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print est.clusters.values()

	estimators_kmeans = {'k_means_k': KMeans(n_clusters=k)}
				#'k_means_15': KMeans(n_clusters=15),
				#'k_means_20': KMeans(n_clusters=20)}
	for name, est in estimators_kmeans.items():
		est.fit(X)
		est.predict(X)
		print name
		print est.labels_
		print est.cluster_centers_
		""" Per ciascun dei centroidi del dpmeans trova quello piu vicino """
	order = pairwise_distances_argmin(estimators_dpmeans['dp_means_5'].clusters.values(),
									  estimators_kmeans['k_means_k'].cluster_centers_)
	print order


def plot_silhouette(s_score, s_values, n_clusters, cluster_labels, X):

	import matplotlib.pyplot as plt
	import matplotlib.cm as cm

	fig, (ax1) = plt.subplots(1, 1)
	fig.set_size_inches(18, 7)

	ax1.set_xlim([-1, 1])
	ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

	y_lower = 10
	for i in range(1,n_clusters+1):
		ith_cluster_silhouette_values = \
			s_values[cluster_labels == i]

		ith_cluster_silhouette_values.sort()

		size_cluster_i = ith_cluster_silhouette_values.shape[0]
		y_upper = y_lower + size_cluster_i

		color = cm.spectral(float(i) / n_clusters)
		ax1.fill_betweenx(np.arange(y_lower, y_upper),
						  0, ith_cluster_silhouette_values,
						  facecolor=color, edgecolor=color, alpha=0.7)

		# Label the silhouette plots with their cluster numbers at the middle
		ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

		# Compute the new y_lower for next plot
		y_lower = y_upper + 10  # 10 for the 0 samples

	ax1.set_title("The silhouette plot for the various clusters.")
	ax1.set_xlabel("The silhouette coefficient values")
	ax1.set_ylabel("Cluster label")

	ax1.axvline(x=s_score, color="red", linestyle="--")
	ax1.set_yticks([])  # Clear the yaxis labels / ticks
	ax1.set_xticks([-1,-0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])

	plt.suptitle(("Silhouette analysis for clustering on sample data "
				  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')
	plt.show()




def main():
	from data_import.ImportDataset import ImportData

	""" Analisi  tramite clustering su dataset globale"""
	""" No diff_Yaw """
	""" Tuning valore di lambda basato su silhouette coefficient"""

	importer = ImportData('') # Non importa il path da rimuovere, inutile
	data_full = pd.read_csv('../xsense_data/global_dataset.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y']]
	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()
	#print 'X', X
	X_np = data_clustering.values
	#print 'X_np', X_np
	print 'reduced'

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)


	estimators_dpmeans = {
	# 					  'dp_means_5': dp_means(X_scaled, _lam=5),
	#					  'dp_means_7': dp_means(X_scaled, _lam=7),
	 					  'dp_means_10': dp_means(X_scaled, _lam=12)
	#					  'dp_means_20': dp_means(X_scaled, _lam=20),
	#					  'dp_means_12': dp_means(X_scaled, _lam=12)
						 }

	for name, est in estimators_dpmeans.items():
		print 'start dpmeans run'
		est.run()
		print 'finished dpmeans run'
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print std_scale.inverse_transform(est.clusters.values())

		""" Add cluster assignments to dataset """
		data_clustering.loc[:, 'Cluster_Label'] = pd.Series(np.array(est.dataClusterId), index= data_clustering.index)
		#(TODO): Save clustered dataset in file

		#Extract just one trip from the full dataset to compute silhouette scores
		trip_id = 15
		df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

		data_clustering_trip = df_trip[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y']]
		print 'data Clustering trip', data_clustering_trip

		X_trip = data_clustering_trip.as_matrix()

		data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
		labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

		print "labels_trip", labels_trip
		print "np array labels trip",  np.array(labels_trip)
		silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

		silhouette_avg = silhouette_score(X, np.array(est.dataClusterId), sample_size= 5000)
		#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

		#print 'Silhouette score ', name, ': ', silhouette_avg

		plot_silhouette(silhouette_avg, silhouette_values_trip, k, np.array(labels_trip), X_trip)

def cluster_abs_diff_yaw():
	import seaborn as sns
	import matplotlib.pyplot as plt

	""" Analisi  tramite clustering su dataset globale con abs su velocita e delta yaw """
	""" Tuning valore di lambda basato su silhouette coefficient """

	data_full = pd.read_csv('../xsense_data/global_dataset_abs_speed_diff_yaw.txt', sep=';')
	print 'imported'

	#(TODO): Reuse Yaw before commmit
	#data_clustering = data_full[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y','Diff_Yaw']]
	data_clustering = data_full[['Acc_X','Acc_Y','Speed_X','Speed_Y','Diff_Yaw']]

	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()
	#print 'X', X
	X_np = data_clustering.values
	#print 'X_np', X_np
	print 'reduced'

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)

	#(TODO): CReate a separeted method
	""" Correlation analysis """
	corr = data_clustering.corr()
	sns.heatmap(corr,square = True)
	plt.yticks(rotation=0)
	plt.xticks(rotation=90)
	plt.show()

	estimators_dpmeans = {
	# 					  'dp_means_5': dp_means(X_scaled, _lam=5),
	#					  'dp_means_7': dp_means(X_scaled, _lam=7),
	 					  'dp_means_10': dp_means(X_scaled, _lam=10),
						  'dp_means_12': dp_means(X_scaled, _lam=12)
	#					  'dp_means_20': dp_means(X_scaled, _lam=20)
						 }

	for name, est in estimators_dpmeans.items():
		print 'start dpmeans run'
		est.run()
		print 'finished dpmeans run'
		k = len(np.unique(est.dataClusterId))
		print 'n_cluster : ',len(np.unique(est.dataClusterId))
		print 'model : ',name
		print 'labels : ',est.dataClusterId
		print 'centroids : ',std_scale.inverse_transform(est.clusters.values())

		""" Add cluster assignments to dataset """
		data_clustering.loc[:, 'Cluster_Label'] = pd.Series(np.array(est.dataClusterId), index= data_clustering.index)
		#(TODO): Save clustered dataset in file

		#Extract just one trip from the full dataset to compute silhouette scores
		trip_id = 26
		df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

		#(TODO): Reuse Yaw before commmit

		data_clustering_trip = df_trip[['Acc_X','Acc_Y','Speed_X','Speed_Y', 'Diff_Yaw']]
		#data_clustering_trip = df_trip[['Acc_X','Acc_Y','Yaw','Speed_X','Speed_Y', 'Diff_Yaw']]
		#print 'data Clustering trip', data_clustering_trip

		X_trip = data_clustering_trip.as_matrix()

		data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
		labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

		print "labels_trip", labels_trip
		print "np array labels trip",  np.array(labels_trip)
		silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

		silhouette_avg = silhouette_score(X, np.array(est.dataClusterId), sample_size= 5000)
		#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

		#print 'Silhouette score ', name, ': ', silhouette_avg

		plot_silhouette(silhouette_avg, silhouette_values_trip, k, np.array(labels_trip), X_trip)

def k_means_clustering(n_cluster):

	data_full = pd.read_csv('../xsense_data/global_dataset_abs_speed_diff_yaw.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X','Acc_Y','Speed_X','Speed_Y','Diff_Yaw']]
	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)
	print 'scaled'

	""" KMeans clustering algorithm"""
	k_means_model = KMeans(n_clusters= n_cluster, init='k-means++')
	k_labels = k_means_model.fit_predict(X_scaled)
	k_centroids = k_means_model.cluster_centers_

	print 'Centroids: ', k_centroids
	""" Add cluster assignments to dataset """
	data_clustering.loc[:, 'Cluster_Label'] = pd.Series(k_labels, index= data_clustering.index)
	#(TODO): Save clustered dataset in file

	#Extract just one trip from the full dataset to compute silhouette scores
	trip_id = 26
	df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

	data_clustering_trip = df_trip[['Acc_X','Acc_Y','Speed_X','Speed_Y', 'Diff_Yaw']]
	#print 'data Clustering trip', data_clustering_trip

	X_trip = data_clustering_trip.as_matrix()

	data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
	labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

	print "labels_trip", labels_trip
	print "np array labels trip",  np.array(labels_trip)
	silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

	silhouette_avg = silhouette_score(X, k_labels, sample_size= 10000)
	#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

	#print 'Silhouette score ', name, ': ', silhouette_avg

	plot_silhouette(silhouette_avg, silhouette_values_trip, n_cluster, np.array(labels_trip), X_trip)

def affinity_propagation_xsens_dataset():

	#Extract just one trip from the full dataset to compute silhouette scores
	trip_id = 26

	data_full = pd.read_csv('../xsense_data/global_dataset_abs_speed_diff_yaw.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X','Acc_Y','Speed_X','Speed_Y','Diff_Yaw']]
	data_clustering = data_clustering.round(5)
	data_clustering = data_clustering.loc[data_full['Trip_ID'] == trip_id]

	X = data_clustering.as_matrix()

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)
	print 'scaled'

	affinity_model = AffinityPropagation(damping = 0.8, preference= -300).fit(X_scaled)
	cluster_centers_indices = affinity_model.cluster_centers_indices_

	labels = affinity_model.labels_
	n_clusters = len(cluster_centers_indices)

	print 'labels: ', labels
	print 'n_clusters: ', n_clusters

	""" Add cluster assignments to dataset """
	data_clustering.loc[:, 'Cluster_Label'] = pd.Series(labels, index= data_clustering.index)
	#(TODO): Save clustered dataset in file

	df_trip = data_full.loc[data_full['Trip_ID'] == trip_id]

	data_clustering_trip = df_trip[['Acc_X','Acc_Y','Speed_X','Speed_Y', 'Diff_Yaw']]
	#print 'data Clustering trip', data_clustering_trip

	X_trip = data_clustering_trip.as_matrix()

	data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
	labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

	print "labels_trip", labels_trip
	print "np array labels trip",  np.array(labels_trip)
	silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

	silhouette_avg = silhouette_score(X_scaled, labels, sample_size= 10000)
	#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

	#print 'Silhouette score ', name, ': ', silhouette_avg

	plot_silhouette(silhouette_avg, silhouette_values_trip, n_clusters, np.array(labels_trip), X_trip)

def dp_means_hcilab_dataset():
	import seaborn as sns
	import matplotlib.pyplot as plt

	""" Analisi  tramite clustering su dataset globale con abs su velocita e delta yaw """
	""" Tuning valore di lambda basato su silhouette coefficient """

	data_full = pd.read_csv('../xsense_data/global_dataset_hcilab_diff_bearing.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]

	data_clustering = data_clustering.round(5)
	X = data_clustering.as_matrix()
	#print 'X', X
	X_np = data_clustering.values
	#print 'X_np', X_np
	print 'reduced'

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)

	#(TODO): CReate a separeted method
	""" Correlation analysis """
	corr = data_clustering.corr()
	sns.heatmap(corr,square = True)
	plt.yticks(rotation=0)
	plt.xticks(rotation=90)
	plt.show()

	estimators_dpmeans = {
	 					  'dp_means_6': dp_means(X_scaled, _lam=6),
						  'dp_means_8': dp_means(X_scaled, _lam=8),
	 					  'dp_means_9': dp_means(X_scaled, _lam=9)
	#					  'dp_means_12': dp_means(X_scaled, _lam=12)
	#					  'dp_means_20': dp_means(X_scaled, _lam=20)
						 }

	for name, est in estimators_dpmeans.items():
		print 'start dpmeans run'
		est.run()
		print 'finished dpmeans run'
		k = len(np.unique(est.dataClusterId))
		print len(np.unique(est.dataClusterId))
		print name
		print est.dataClusterId
		print std_scale.inverse_transform(est.clusters.values())

		""" Add cluster assignments to dataset """
		data_clustering.loc[:, 'Cluster_Label'] = pd.Series(np.array(est.dataClusterId), index= data_clustering.index)
		#(TODO): Save clustered dataset in file

		#Extract just one trip from the full dataset to compute silhouette scores
		trip_id = 5
		df_trip = data_full.loc[data_full['Trip_ID']== trip_id]

		#(TODO): Reuse Yaw before commmit

		data_clustering_trip = df_trip[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]

		X_trip = data_clustering_trip.as_matrix()

		data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
		labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

		print "labels_trip", labels_trip
		print "np array labels trip",  np.array(labels_trip)
		silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

		silhouette_avg = silhouette_score(X, np.array(est.dataClusterId), sample_size= 5000)
		#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

		print 'Silhouette score ', name, ': ', silhouette_avg

		plot_silhouette(silhouette_avg, silhouette_values_trip, k, np.array(labels_trip), X_trip)

def affinity_propagation_hcilab_dataset():
	trip_id = 5

	data_full = pd.read_csv('../xsense_data/global_dataset_hcilab_diff_bearing.txt', sep=';')
	print 'imported'

	data_clustering = data_full[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]
	data_clustering = data_clustering.round(5)
	data_clustering = data_clustering.loc[data_full['Trip_ID'] == trip_id]

	X = data_clustering.as_matrix()

	std_scale = preprocessing.StandardScaler().fit(X)
	X_scaled = std_scale.transform(X)
	print 'scaled'

	affinity_model = AffinityPropagation(damping = 0.8, preference= -200).fit(X_scaled)
	cluster_centers_indices = affinity_model.cluster_centers_indices_

	labels = affinity_model.labels_
	n_clusters = len(cluster_centers_indices)
	centroids = std_scale.inverse_transform(affinity_model.cluster_centers_)

	print 'labels: ', labels
	print 'n_clusters: ', n_clusters
	print 'centroids: ', centroids

	""" Add cluster assignments to dataset """
	data_clustering.loc[:, 'Cluster_Label'] = pd.Series(labels, index= data_clustering.index)
	#(TODO): Save clustered dataset in file

	df_trip = data_full.loc[data_full['Trip_ID'] == trip_id]

	data_clustering_trip = df_trip[['Acc_X', 'Acc_Z', 'Speed', 'Diff_Bearing']]
	#print 'data Clustering trip', data_clustering_trip

	X_trip = data_clustering_trip.as_matrix()

	data_clustering_trip_labels = data_clustering.loc[data_full['Trip_ID'] == trip_id]
	labels_trip = data_clustering_trip_labels.ix[:,'Cluster_Label'].tolist()

	print "labels_trip", labels_trip
	silhouette_values_trip = silhouette_samples(X_trip, np.array(labels_trip))

	silhouette_avg = silhouette_score(X_scaled, labels, sample_size= 10000)
	#silhouette_values = silhouette_samples(X, np.array(est.dataClusterId))

	print 'Silhouette score : ', silhouette_avg

	plot_silhouette(silhouette_avg, silhouette_values_trip, n_clusters, np.array(labels_trip), X_trip)

if __name__ == "__main__":
	#main()
	#cluster_abs_diff_yaw()
	#k_means_clustering(5)
	#k_means_clustering(7)
	#k_means_clustering(9)

	#affinity_propagation_xsens_dataset()

	#dp_means_hcilab_dataset()
	affinity_propagation_hcilab_dataset()